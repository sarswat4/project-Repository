{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "JMzcOPDDphqR",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "gIfDvo9L0UH2"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarswat4/project-Repository/blob/main/Copy_of_Netflix_Movies_and_Tv_Show_Project_ML_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV Shows: Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Type**    - Unsupervised\n",
        "\n",
        "**Contribution**    - Individual\n",
        "\n",
        "**Team Member 1 -** Diksha Sarswat"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Problem statement\n",
        "2. Importing the libraries and the dataset\n",
        "3. Understanding data\n",
        "4. Cleaning data\n",
        "5. Exploratory data analysis\n",
        "6. Data preprocessing\n",
        "7. Clusters impelementation\n",
        "8. Building content based recommender system\n",
        "9. Conclusions\n",
        "10. References\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This project focuses on analyzing a dataset of TV shows and movies available on Netflix as of 2019, sourced from Flixable, a third-party Netflix search engine.\n",
        "\n",
        "* In 2018, Flixable published a report revealing that the number of TV shows on Netflix has almost tripled since 2010, while the number of movies has declined by over 2,000 titles during the same period. This trend presents an intriguing opportunity to explore further insights from the dataset.\n",
        "\n",
        "* We will be clustering similar content by matching text-based features."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Data Maipulation Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "# Data Visualisation Libraray\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Importing libraries for hypothesis testing\n",
        "from scipy.stats import uniform\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import chi2\n",
        "from scipy.stats import t\n",
        "from scipy.stats import f\n",
        "from scipy.stats import ttest_ind\n",
        "import scipy.stats as stats\n",
        "# libraries used to process textual data\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# libraries used to implement clusters\n",
        "from sklearn.metrics import silhouette_score\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "# Library of warnings would assist in ignoring warnings issued\n",
        "import warnings;warnings.filterwarnings('ignore')\n",
        "import warnings;warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "path = '/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING (1).csv'\n",
        "dff=pd.read_csv(path,encoding=\"ISO-8859-1\")"
      ],
      "metadata": {
        "id": "UA1a2BdNmT1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "dff.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "dff.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "dff.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "dff.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "dff.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10,7))\n",
        "ax=sns.barplot(x=dff.columns,y=dff.isna().sum(),palette='Set2')\n",
        "for label in ax.containers:\n",
        "    ax.bar_label(label)\n",
        "plt.grid(alpha=0.5)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The dataset contains information about movies and TV shows available on Netflix up until 2019.\n",
        "*   The dataset has 7787 entries and 12 columns. Out of the 12 columns, 11 are of 'object' datatype and 1 is of 'numeric' datatype.\n",
        "*There are no duplicate values in the dataset.\n",
        "*There are five columns containing missing values. There are total 3631 missing values present in the dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dff.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dff.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. show_id: Unique ID for every Movie/TV show\n",
        "\n",
        "2. type: A movie or a TV show\n",
        "3.title: Title of the Movie/TV show\n",
        "4.director: Director of the Movie/TV show\n",
        "5.cast: Actors in the movie\n",
        "6.country: Country of produciton\n",
        "7.date_added: Date it was added on Netflix\n",
        "8.release_year: Actual release year of the Movie/TV show\n",
        "9.rating: Movie/TV show rating\n",
        "10.duration: duration in minutes/number of seasons\n",
        "11.listed_in: Genre\n",
        "12.Description: The description summary"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "dff.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Handling null values\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bKYRGW5CLian"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "dff.isna().sum()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff[['director','cast']]=dff[['director','cast']].fillna('Unknown')\n",
        "dff['country']=dff['country'].fillna(dff['country'].mode()[0])\n",
        "dff.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "yiSlYosfMQSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff.shape"
      ],
      "metadata": {
        "id": "KGrY-BtnMQN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff.duplicated().sum()"
      ],
      "metadata": {
        "id": "ExrIaYH_MQJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   To handle null values in the 'director' and 'cast' columns, we can replace them with the value 'Unknown'. This ensures that missing information in these columns is still captured in the dataset.\n",
        "*  For the 'country' column, we can fill the null values with the country that appears most frequently in the dataset, using the mode.\n",
        "\n"
      ],
      "metadata": {
        "id": "hCJB21TLNB8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Handling nested columns i.e 'director', 'cast', 'listed_in' and 'country'"
      ],
      "metadata": {
        "id": "dk4F2pk5NQ2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a copy of dataframe and unnest the original one\n",
        "dff_new= dff.copy()"
      ],
      "metadata": {
        "id": "ypXYqMxVNYwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'Directors' column\n",
        "dir=dff['director'].apply(lambda x: str(x).split(',')).tolist()\n",
        "df1=pd.DataFrame(dir,index=dff['title'])\n",
        "df1=df1.stack()\n",
        "df1=pd.DataFrame(df1.reset_index())\n",
        "df1.rename(columns={0:'Directors'},inplace=True)\n",
        "df1 = df1.drop(['level_1'],axis=1)\n",
        "df1.sample(10)"
      ],
      "metadata": {
        "id": "4i7Z3DVMNcr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "id": "5zwmr8M2N0T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'cast' column\n",
        "cast=dff['cast'].apply(lambda x: str(x).split(',')).tolist()\n",
        "df2=pd.DataFrame(cast,index=dff['title'])\n",
        "df2=df2.stack()\n",
        "df2=pd.DataFrame(df2.reset_index())\n",
        "df2.rename(columns={0:'Actors'},inplace=True)\n",
        "df2 = df2.drop(['level_1'],axis=1)\n",
        "df2.sample(10)"
      ],
      "metadata": {
        "id": "nqgYPmTRN1cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "id": "r5gtKb1OOlB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'listed_in' column\n",
        "lst=dff['listed_in'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df3 = pd.DataFrame(lst, index = dff['title'])\n",
        "df3 = df3.stack()\n",
        "df3 = pd.DataFrame(df3.reset_index())\n",
        "df3.rename(columns={0:'Genre'},inplace=True)\n",
        "df3 = df3.drop(['level_1'],axis=1)\n",
        "df3.sample(10)"
      ],
      "metadata": {
        "id": "di9y5tJWOqGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.shape"
      ],
      "metadata": {
        "id": "03SN6JIlO_s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'country' column\n",
        "country=dff['country'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df4 = pd.DataFrame(country, index = dff['title'])\n",
        "df4 = df4.stack()\n",
        "df4 = pd.DataFrame(df4.reset_index())\n",
        "df4.rename(columns={0:'Country'},inplace=True)\n",
        "df4 = df4.drop(['level_1'],axis=1)\n",
        "df4.sample(10)"
      ],
      "metadata": {
        "id": "Kx9N24jKPA7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4.shape"
      ],
      "metadata": {
        "id": "6T-t6s8zPIP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging all the unnested dataframes\n",
        "# Merging director and cast\n",
        "df5 = df2.merge(df1,on=['title'],how='inner')\n",
        "\n",
        "# Merging listed_in with merged of (director and cast)\n",
        "df6 = df5.merge(df3,on=['title'],how='inner')\n",
        "\n",
        "# Merging country with merged of [listed_in with merged of (director and cast)]\n",
        "df7 = df6.merge(df4,on=['title'],how='inner')\n",
        "\n",
        "# Head of final merged dataframe\n",
        "df7.head()"
      ],
      "metadata": {
        "id": "qY4RGrBkPR5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final DataFrame:\n",
        "dff=df7.merge(dff[['type', 'title', 'date_added', 'release_year', 'rating', 'duration','description']],on=['title'],how='left')\n",
        "dff.head()"
      ],
      "metadata": {
        "id": "AaMn2qVbPUoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff.shape"
      ],
      "metadata": {
        "id": "0V2gxz8fPd98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Changing datatype"
      ],
      "metadata": {
        "id": "1Ffj8MsZP4mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dff.info()"
      ],
      "metadata": {
        "id": "nnlCxeL4QBIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'date_added' column:\n",
        "dff['date_added'] = pd.to_datetime(dff['date_added'], format='%B %d, %Y', errors='coerce')\n",
        "# Extracting day, month and year from date_added column\n",
        "dff[\"day_added\"] = dff[\"date_added\"].dt.day\n",
        "dff[\"month_added\"] = dff[\"date_added\"].dt.month\n",
        "dff[\"year_added\"] = dff[\"date_added\"].dt.year"
      ],
      "metadata": {
        "id": "j9Tiq2YIyKuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping date_added\n",
        "dff.drop('date_added', axis=1, inplace=True)\n",
        "\n",
        "# removing 'min' and 'Seasons' from the end in 'duration' column:\n",
        "dff['duration']= dff['duration'].apply(lambda x: int(x.split()[0]))\n"
      ],
      "metadata": {
        "id": "v0igJtQLQF5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff.info()"
      ],
      "metadata": {
        "id": "ZgqRE9NHQUiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Binning of Rating variable\n",
        "\n",
        "In rating columns we have different categories these are content rating classifications that are commonly used in the United States and other countries to indicate the appropriateness of media content for different age groups. Let's understand each of them and binnig them accordingly:\n",
        "\n",
        "TV-MA: This rating is used for mature audiences only, and it may contain strong language, violence, nudity, and sexual content.\n",
        "\n",
        "R: This rating is used for movies that are intended for audiences 17 and older. It may contain graphic violence, strong language, drug use, and sexual content.\n",
        "\n",
        "PG-13: This rating is used for movies that may not be suitable for children under 13. It may contain violence, mild to moderate language, and suggestive content.\n",
        "\n",
        "TV-14: This rating is used for TV shows that may not be suitable for children under 14. It may contain violence, strong language, sexual situations, and suggestive dialogue.\n",
        "\n",
        "TV-PG: This rating is used for TV shows that may not be suitable for children under 8. It may contain mild violence, language, and suggestive content.\n",
        "\n",
        "NR: This stands for \"Not Rated.\" It means that the content has not been rated by a rating board, and it may contain material that is not suitable for all audiences.\n",
        "\n",
        "TV-G: This rating is used for TV shows that are suitable for all ages. It may contain some mild violence, language, and suggestive content.\n",
        "\n",
        "TV-Y: This rating is used for children's TV shows that are suitable for all ages. It is intended to be appropriate for preschool children.\n",
        "\n",
        "TV-Y7: This rating is used for children's TV shows that may not be suitable for children under 7. It may contain mild violence and scary content.\n",
        "\n",
        "PG: This rating is used for movies that may not be suitable for children under 10. It may contain mild language, some violence, and some suggestive content.\n",
        "\n",
        "G: This rating is used for movies that are suitable for general audiences. It may contain some mild language and some violence.\n",
        "\n",
        "NC-17: This rating is used for movies that are intended for adults only. It may contain explicit sexual content, violence, and language.\n",
        "\n",
        "TV-Y7-FV: This rating is used for children's TV shows that may not be suitable for children under 7. It may contain fantasy violence.\n",
        "\n",
        "UR: This stands for \"Unrated.\" It means that the content has not been rated by a rating board, and it may contain material that is not suitable for all audiences.\n",
        "\n",
        "Let's not complicate it and create bins as following:\n",
        "\n",
        "Adult Content: TV-MA, NC-17, R\n",
        "\n",
        "Children Content: TV-PG, PG, TV-G, G\n",
        "\n",
        "Teen Content: PG-13, TV-14\n",
        "\n",
        "Family-friendly Content: TV-Y, TV-Y7,TV-Y7-FV\n",
        "\n",
        "Not Rated: NR, UR\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j-C2dkNbQ1BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning the values in the rating column\n",
        "rating_map = {'TV-MA':'Adult Content',\n",
        "              'R':'Adult Content',\n",
        "              'PG-13':'Teen Content',\n",
        "              'TV-14':'Teen Content',\n",
        "              'TV-PG':'Children Content',\n",
        "              'NR':'Not Rated',\n",
        "              'TV-G':'Children Content',\n",
        "              'TV-Y':'Family-friendly Content',\n",
        "              'TV-Y7':'Family-friendly Content',\n",
        "              'PG':'Children Content',\n",
        "              'G':'Children Content',\n",
        "              'NC-17':'Adult Content',\n",
        "              'TV-Y7-FV':'Family-friendly Content',\n",
        "              'UR':'Not Rated'}\n",
        "\n",
        "dff['rating'].replace(rating_map, inplace = True)\n",
        "dff['rating'].unique()"
      ],
      "metadata": {
        "id": "h207Jmk4RWW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff.sample(3)"
      ],
      "metadata": {
        "id": "JjlEvDJORfZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Separating Movies and TV Shows\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SS_M_P7URmPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spearating the dataframes for further analysis\n",
        "dff_movies= dff[dff['type']== 'Movie']\n",
        "dff_tvshows= dff[dff['type']== 'TV Show']\n",
        "\n",
        "# Printing the shape\n",
        "print(dff_movies.shape, dff_tvshows.shape)"
      ],
      "metadata": {
        "id": "Lb8IPLVDRtEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have divided data wrangling into five different sections:\n",
        "\n",
        "1. In this section we have imputed/drop the null values of:\n",
        "\n",
        "* Imputed 'director' and 'cast' with 'Unknown'.\n",
        "*Imputed 'country' with Mode.\n",
        "*Drop null values of 'date_added' and 'rating'.\n",
        "\n",
        "2. We have unnested values from following features:\n",
        "\n",
        " 'director'\n",
        "\n",
        " 'cast'\n",
        "\n",
        " 'listed_in'\n",
        "\n",
        " 'country'\n",
        "\n",
        " We have unnested the values and stored in different dataframes and then merged all the dataframe with the original one using left join in order to get the isolated values of each of the feature.\n",
        "\n",
        "3. We have changed the data type of following features:\n",
        "* 'duration' into integer (Removing min and seasons from the values).\n",
        "* 'date_added' to datetime (Into the required format).\n",
        "** We have also extracted the following features:\n",
        "\n",
        " 'date' from 'date_added'.\n",
        "\n",
        " 'month' from 'date_added'.\n",
        "\n",
        " 'year' from 'date_added'.\n",
        "* We have seen that the 'rating' column contains various coded categories, so we have decided to create 5 bins and distribute the values accordingly:\n",
        "\n",
        " Adult: TV-MA, NC-17\n",
        "\n",
        " Restricted: R, UR\n",
        "\n",
        " Teen: PG-13, TV-14\n",
        "\n",
        " All Ages: TV-G, TV-Y, TV-Y7, TV-Y7-FV, PG, G, TV-PG\n",
        "\n",
        " Not Rated: NR\n",
        "\n",
        "5. Lastly we have splitted the dataframe into two df one is 'dff_movies' that contains only Movies and the other is 'dff_tvshows' that contains only TV Shows for our further analysis."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Plotting a piechart for categorical variable- 'type'\n",
        "\n",
        "plt.pie(dff['type'].value_counts(),labels = dff['type'].value_counts().keys().tolist(),autopct='%.0f%%')\n",
        "plt.title('type')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Pie charts are one of the best ways for univariate analysis of categorical data."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our dataset, we have 72% of them as movies, the rest 28% is TV shows.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The insight that there are more movies on Netflix than TV shows is unlikely to have a significant positive or negative business impact on its own. However, this information could be used in conjunction with other insights and data to inform business decisions.\n",
        "\n",
        " For example, if Netflix notices that TV shows are more popular with its subscribers than movies, it may decide to focus more on acquiring TV show content. Alternatively, if it sees that its original movie productions are gaining popularity, it may decide to invest more in that area.\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(20, 10))\n",
        "datasets = [(dff, 'Overall'), (dff_movies, 'Movies'), (dff_tvshows, 'TV Shows')]\n",
        "\n",
        "for i, (data, label) in enumerate(datasets):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    count = data['rating'].value_counts()\n",
        "    plt.pie(count, labels=count.index, autopct='%1.1f%%',explode=(0,0,0,0,0.5),colors=sns.color_palette(\"Set2\"),shadow=True)\n",
        "    plt.title(f\"Distribution of Content Rating on Netflix '{label}'\")\n",
        "    plt.axis('equal')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart, was utilized to analyze the distribution of content types within Netflix. Its purpose was to determine the percentage of each content type (movies and TV shows) present on the platform."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The analysis revealed that the majority of content available on Netflix falls under the Adult and Teen categories. These two categories constitute a significant portion of the content library.\n",
        "\n",
        "* A noteworthy finding is that, in comparison to TV shows, there is a relatively lower presence of family-friendly content in the Movies category on Netflix. This suggests that TV shows offer a greater selection of content suitable for family viewing.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights regarding the content distribution on Netflix, specifically the prevalence of Adult and Teen categories and the disparity in family-friendly content between Movies and TV Shows, can potentially create a positive business impact. By catering to the preferences of their target audience and strategically addressing the content gaps, Netflix can enhance user satisfaction, attract new subscribers, and ultimately drive business growth and success."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(22,8))\n",
        "for i,j,k in (dff_movies,'Movies',1),(dff_tvshows,'TV Shows',2):\n",
        "  plt.subplot(1,2,k)\n",
        "  k=sns.barplot(y='Directors',x='title',data=i.groupby(['Directors']).nunique().sort_values(by='title',ascending=False).reset_index()[1:11],palette='Set2')\n",
        "  for label in k.containers:\n",
        "            k.bar_label(label)\n",
        "  plt.grid(alpha=0.3)\n",
        "  plt.title(f'Top 10 Directors with maximum number of  {j}')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know about the most popular Directors in Movies and TV shows."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We found that most of the movies directed by Raul Campos and Jan Suter.\n",
        "\n",
        "* Most TV shows directed by Alastair Fothergill and ken burns."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movie/tvshows producers can select the popular director for their upcoming projects.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(22,8))\n",
        "for i,j,k in (dff_movies,'Movies',1),(dff_tvshows,'TV Shows',2):\n",
        "  plt.subplot(1,2,k)\n",
        "  k=sns.barplot(y='Actors',x='title',data=i.groupby('Actors').nunique().sort_values(by='title',ascending=False).reset_index()[1:11],palette='Set2')\n",
        "  for label in k.containers:\n",
        "            k.bar_label(label)\n",
        "  plt.grid(alpha=0.3)\n",
        "  plt.title(f'Top 10 Actors with maximum number of  {j}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which actors are more popular on Netflix."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Majority of movie actors are from India, but no popular Indian actors found in TV shows."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indian audience's passion for movies makes them a lucrative target market. Businesses should focus on capturing their attention and preferences to maximize viewership and commercial success."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(25,8))\n",
        "for i,j,k in (dff_movies,'Movies',1),(dff_tvshows,'TV Shows',2):\n",
        "  plt.subplot(1,2,k)\n",
        "  k=sns.barplot(y='Genre',x='title',data=i.groupby('Genre').nunique().sort_values(by='title',ascending=False).reset_index()[:10],palette='Set2')\n",
        "  for label in k.containers:\n",
        "            k.bar_label(label)\n",
        "  plt.grid(alpha=0.3)\n",
        "  plt.title(f'Top 10 Genre in  {j}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which genre is more popular w.r.t movies/TV shows on Netflix"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "International movies/TV shows genre is most popular in both the Movies and TV Shows category. Followed by Drama and comedy."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights about the popularity of international movies/TV shows, as well as drama and comedy genres, can potentially create a positive business impact. By aligning content production and marketing strategies to cater to these preferences, businesses can attract a larger audience and potentially boost profitability and market share."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "plt.figure(figsize=(18,8))\n",
        "ax=dff.groupby(['Country'])['title'].nunique().sort_values(ascending=False)[:10].plot(kind='bar',color=sns.color_palette('Set2'))\n",
        "for label in ax.containers:\n",
        "            ax.bar_label(label)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.title('Top 10 countries with most number of  contents on Netflix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which country contribute  maximum number of contents on Netflix."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "United States at the top followed by India in terms of most number of content contributor on Netflix."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights about the United States and India being the top contributors of content on Netflix can have a positive business impact. Understanding the content origins allows businesses to tailor their strategies to effectively cater to the preferences of these countries, potentially leading to increased viewership, subscriber retention, and revenue generation in these key markets."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(20,8))\n",
        "ax=sns.barplot(x='year_added',y='title',data=dff.groupby(['year_added','type']).agg({'title':'nunique'}).reset_index().sort_values(by='title',ascending=False),hue='type',palette='Set2')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.title('Type of Content added each year on Netflix')\n",
        "plt.ylabel('Count')\n",
        "for label in ax.containers:\n",
        "            ax.bar_label(label)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know about the Type of Content added each year on Netflix."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In terms of content additions, the year 2019 witnessed the highest number of movies being added to the platform, totaling 1497. Conversely, the year 2020 stands out for having the highest number of TV shows added, with a substantial count of 697. These insights highlight the significant growth and diversity of content offerings during those respective years.\n",
        "\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights about the highest number of movie and TV show additions in specific years can potentially create a positive business impact. By leveraging these trends, businesses can strategically plan their content acquisition and production strategies, ensuring a diverse and appealing library that caters to viewer preferences and drives engagement and subscriber growth.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(20,8))\n",
        "ax=sns.lineplot(x='month_added',y='title',data=dff.groupby(['month_added','type']).agg({'title':'nunique'}).reset_index().sort_values(by='title',ascending=False),hue='type',palette='Set2', marker = 'o')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.title('Type of Content added w.r.t month  on Netflix')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot show Count of Movie/TV shows added each month."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Starting of Year(Jan) and End of Year(Dec) shows a high count of Movies added to Netflix.\n",
        "* Month of December shows high count of TV shows added to Netflix"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights that show a high count of movies added to Netflix at the start and end of the year, as well as a high count of TV shows added in December, can inform strategic content acquisition decisions, leading to increased user engagement and subscriber retention.\n",
        "\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "df1=dff_movies.groupby('title').agg({'duration':'unique'}).reset_index()\n",
        "df1['duration']=df1['duration'].apply(lambda x: x[0])\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.title('Histplot  for Movies')\n",
        "plt.hist(df1['duration'],color='orange',bins=30)\n",
        "plt.xlabel('Duration')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "To Know about the duration distribution of movies."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the Movies on Netflix fall under category of duration between 90 min to 120 min."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights about the majority of movies on Netflix falling within the duration of 90 to 120 minutes can help in curating and acquiring content that aligns with user preferences, leading to increased viewer satisfaction and engagement, thereby potentially creating a positive business impact."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Preparing data for heatmap\n",
        "dff['count'] = 1\n",
        "dff_heatmap = pd.crosstab(dff['Country'], dff['rating'], normalize=\"index\").T\n",
        "\n",
        "# Plotting the heatmap\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "\n",
        "# Defining order of representation\n",
        "country_order = ['United States', 'India', 'United Kingdom', 'Canada', 'France', 'Japan', 'Spain', 'South Korea', 'Germany', 'Mexico']\n",
        "# Get all unique rating values from the index that are present in the DataFrame\n",
        "rating_order = [rating for rating in dff_heatmap.index if rating in dff['rating'].unique()]\n",
        "\n",
        "# Ensure all countries in country_order are present in the columns\n",
        "country_order = [country for country in country_order if country in dff_heatmap.columns]\n",
        "\n",
        "# calling and plotting heatmap\n",
        "sns.heatmap(dff_heatmap.loc[rating_order, country_order], cmap='Set2', square=True, linewidth=2.5, cbar=False, annot=True, fmt='1.0%', vmax=.6, vmin=0.05, ax=ax, annot_kws={\"fontsize\":12})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us which countries producing which type of content the most."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* All countries except India producing Adult Content in higher proportion.\n",
        "* India producing high percentage of Teen Content.\n",
        "* Canada is in Top in terms of Family-friendly Content (if we compares to others).\n",
        "* 85% of content is Adult content from spain."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.pairplot(dff, corner=True)"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Null Hypothesis:** There is no significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "* **Alternative Hypothesis:** There is a significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "# Subset the data to only include drama and comedy movies\n",
        "subset = dff[dff['Genre'].str.contains('Dramas') | dff['Genre'].str.contains('Comedies')]\n",
        "\n",
        "# Calculate the proportion of drama and comedy movies\n",
        "drama_prop = len(subset[subset['Genre'].str.contains('Dramas')]) / len(subset)\n",
        "comedy_prop = len(subset[subset['Genre'].str.contains('Comedies')]) / len(subset)\n",
        "\n",
        "# Set up the parameters for the z-test\n",
        "count = [int(drama_prop * len(subset)), int(comedy_prop * len(subset))]\n",
        "nobs = [len(subset), len(subset)]\n",
        "alternative = 'two-sided'\n",
        "\n",
        "# Perform the z-test\n",
        "z_stat, p_value = proportions_ztest(count = count, nobs = nobs, alternative = alternative)\n",
        "print('z-statistic: ', z_stat)\n",
        "print('p-value: ', p_value)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results of the z-test\n",
        "if p_value < alpha:\n",
        "    print(f\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test we have used to obtain the P-value is the z-test for proportions."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The z-test for proportions is utilized when we need to compare the proportions of two categorical variables, specifically drama movies and comedy movies, within a given sample. The objective is to assess whether the observed disparity in proportions is statistically significant or merely due to chance. This statistical test is chosen because it enables us to evaluate the likelihood of observing the observed difference in proportions in our sample, assuming the null hypothesis to be true. In simpler terms, it allows us to determine if the difference between the two proportions is meaningful or just a result of random variation.\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* **Null Hypothesis:** The average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "* **Alternative Hypothesis**: The average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# To test this hypothesis, we perform a two-sample t-test.\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Create separate dataframes for TV shows in 2020 and 2021\n",
        "tv_2020 = dff[(dff['type'] == 'TV Show') & (dff['release_year'] == 2020)]\n",
        "tv_2021 = dff[(dff['type'] == 'TV Show') & (dff['release_year'] == 2021)]\n",
        "\n",
        "# Extract numerical duration values before applying t-test\n",
        "tv_2020_duration = tv_2020['duration'].astype(str).str.extract('(\\d+)').astype(int)[0] # Extract numerical part and convert to int\n",
        "tv_2021_duration = tv_2021['duration'].astype(str).str.extract('(\\d+)').astype(int)[0] # Extract numerical part and convert to int\n",
        "\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t, p = ttest_ind(tv_2020_duration,\n",
        "                 tv_2021_duration, equal_var=False, nan_policy='omit') #added nan_policy = 'omit' to handle NaN values\n",
        "print('t-value: ', t)\n",
        "print('p-value: ', p)\n",
        "\n",
        "# Print the results\n",
        "if p < 0.05:\n",
        "    print('Reject null hypothesis. \\nThe average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021.')\n",
        "else:\n",
        "    print('Failed to reject null hypothesis. \\nThe average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.')"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used to obtain the P-Value is a two-sample t-test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test was chosen because we are comparing the means of two different samples (TV shows added in 2020 vs TV shows added in 2021) to determine whether they are significantly different. Additionally, we assume that the two samples have unequal variances since it is unlikely that the duration of TV shows added in 2020 and 2021 would have the exact same variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of movies on Netflix is greater than the number of TV shows.\n",
        "\n",
        "* **Null hypothesis H0**: The number of movies and TV shows on Netflix is not significantly different.\n",
        "\n",
        "* **Alternate hypothesis Ha:** The number of movies on Netflix is significantly greater than the number of TV shows.\n",
        "\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "# Count the number of movies and TV shows\n",
        "n_movies = dff[dff['type'] == 'Movie'].count()['type']\n",
        "n_tv_shows = dff[dff['type'] == 'TV Show'].count()['type']\n",
        "\n",
        "# Set the counts and sample sizes for the z-test\n",
        "counts = [n_movies, n_tv_shows]\n",
        "nobs = [len(dff), len(dff)]\n",
        "\n",
        "# Perform the z-test assuming equal proportions\n",
        "z_stat, p_val = proportions_ztest(counts, nobs, value=0, alternative='larger')\n",
        "\n",
        "# Print the results\n",
        "print('Number of movies:', n_movies)\n",
        "print('Number of TV shows:', n_tv_shows)\n",
        "print('z-statistic:', z_stat)\n",
        "print('p-value:', p_val)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a two-sample z-test for proportions to obtain the p-value. The null hypothesis for the test is that the proportion of movies and TV shows on Netflix is equal, while the alternative hypothesis is that the proportion of movies is greater than the proportion of TV shows.\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose the two-sample z-test for proportions to compare the number of movies and TV shows on Netflix because the data consists of two categorical variables (Movies and TV show), and we want to test if there is a significant difference between the proportions of these categories in the population. The two-sample z-test for proportions is an appropriate test to use when we have two independent samples, and we want to compare the proportion of successes in each sample. In this case, a success refers to a movie or TV show. The test assumes that the samples are large enough to apply the normal approximation to the binomial distribution. Since we have a large sample size in this case, we can use the z-test for proportions to test the hypothesis of interest."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "dff.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used mode imputation for categorical columns to preserve distribution, and median/mean imputation for numeric columns to avoid bias from outliers."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "numeric_features=[\"release_year\",\"duration\",\"day_added\",\"month_added\",\"year_added\"]\n",
        "plt.figure(figsize=(20,5))\n",
        "for i,column in enumerate(numeric_features):\n",
        "  plt.subplot(1,5,i+1)\n",
        "  sns.boxplot(dff[column],palette='Set2')\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " usesed boxplot visualization to identify potential outliers, allowing for manual detection of extreme values that can be addressed using techniques like removal or capping.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dff_new.sample(5)"
      ],
      "metadata": {
        "id": "md3I7hUgexFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff.shape, dff_new.shape"
      ],
      "metadata": {
        "id": "7zfyUWx5ezxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Binning of rating in new dataframe\n",
        "rating_map = {'TV-MA':'Adult Content',\n",
        "              'R':'Adult Content',\n",
        "              'PG-13':'Teen Content',\n",
        "              'TV-14':'Teen Content',\n",
        "              'TV-PG':'Children Content',\n",
        "              'NR':'Not Rated',\n",
        "              'TV-G':'Children Content',\n",
        "              'TV-Y':'Family-friendly Content',\n",
        "              'TV-Y7':'Family-friendly Content',\n",
        "              'PG':'Children Content',\n",
        "              'G':'Children Content',\n",
        "              'NC-17':'Adult Content',\n",
        "              'TV-Y7-FV':'Family-friendly Content',\n",
        "              'UR':'Not Rated'}\n",
        "\n",
        "dff_new['rating'].replace(rating_map, inplace = True)\n",
        "dff_new['rating'].unique()"
      ],
      "metadata": {
        "id": "V25Jk6Y8fLrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Textual Columns\n",
        "\n"
      ],
      "metadata": {
        "id": "GIH3Z-uPgufb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new feature content_detail with the help of other textual attributes\n",
        "dff_new[\"content_detail\"]= dff_new[\"cast\"]+\" \"+dff_new[\"director\"]+\" \"+dff_new[\"listed_in\"]+\" \"+dff_new[\"type\"]+\" \"+dff_new[\"rating\"]+\" \"+dff_new[\"country\"]+\" \"+dff_new[\"description\"]\n",
        "\n",
        "#checking the manipulation\n",
        "dff_new.head(5)"
      ],
      "metadata": {
        "id": "0Iei8RDfhB8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "dff_new['content_detail']= dff_new['content_detail'].str.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "# Remove Punctuations\n",
        "def remove_punctuations(text):\n",
        "    '''This function is used to remove the punctuations from the given sentence'''\n",
        "    #imorting needed library\n",
        "    import string\n",
        "    # replacing the punctuations with no space, which in effect deletes the punctuation marks.\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped off punctuation marks\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Punctuations from the content_detail\n",
        "dff_new['content_detail']= dff_new['content_detail'].apply(remove_punctuations)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "dff_new.iloc[123,]['content_detail']"
      ],
      "metadata": {
        "id": "xzi4XoNZiMDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_url_and_numbers(text):\n",
        "    '''This function is used to remove the URL's and Numbers from the given sentence'''\n",
        "    # importing needed libraries\n",
        "    import re\n",
        "    import string\n",
        "\n",
        "    # Replacing the URL's with no space\n",
        "    url_number_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text= re.sub(url_number_pattern,'', text)\n",
        "\n",
        "    # Replacing the digits with one space\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # return the text stripped off URL's and Numbers\n",
        "    return text"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Remove URLs & Remove words and digits contain digits\n",
        "dff_new['content_detail']= dff_new['content_detail'].apply(remove_url_and_numbers)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "dff_new.iloc[123,]['content_detail']"
      ],
      "metadata": {
        "id": "mdD0medTilM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def remove_stopwords_and_whitespaces(text):\n",
        "    '''This function is used for removing the stopwords from the given sentence'''\n",
        "    text = [word for word in text.split() if not word in stopwords.words('english')]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    text=  \" \".join(text)\n",
        "\n",
        "    # removing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # return the manipulated string\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff_new['content_detail']= dff_new['content_detail'].apply(remove_stopwords_and_whitespaces)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "dff_new.iloc[123,]['content_detail']"
      ],
      "metadata": {
        "id": "biMbgTgrjB4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')  # Download the required data for tokenization\n",
        "# Checking the observation after manipulation\n",
        "dff_new.iloc[123,]['content_detail']"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading needed libraries\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Creating instance for wordnet\n",
        "wordnet  = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "f9uOoLtHlTq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's the lemmatizing function. Ensure this cell is run before the one calling it.\n",
        "def lemmatizing_sentence(text):\n",
        "    '''This function is used for lemmatizing (changing the given word into meaningful word) the words from the given sentence'''\n",
        "    text = [wordnet.lemmatize(word) for word in text]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    text=  \" \".join(text)\n",
        "\n",
        "    # return the manipulated string\n",
        "    return text\n",
        "\n",
        "# ... (Your other code, including imports, etc.)\n",
        "\n",
        "# Now apply the function:\n",
        "dff_new['content_detail']= dff_new['content_detail'].apply(lemmatizing_sentence)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrasing text by applying defined lemmatizing function\n",
        "dff_new['content_detail']= dff_new['content_detail'].apply(lemmatizing_sentence)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "dff_new.iloc[123,]['content_detail']"
      ],
      "metadata": {
        "id": "7Wdf10DblazP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " We have used Lemmatization instead of Stemming for our project because:\n",
        "\n",
        "* Lemmatization produces a more accurate base word: Unlike Stemming, which simply removes the suffix from a word, Lemmatization looks at the meaning of the word and its context to produce a more accurate base form.\n",
        "\n",
        "* Lemmatization can handle different inflections: Lemmatization can handle various inflections of a word, including plural forms, verb tenses, and comparative forms, making it useful for natural language processing.\n",
        "\n",
        "* Lemmatization produces real words: Lemmatization always produces a real word that can be found in a dictionary, making it easier to interpret the results of text analysis.\n",
        "\n",
        "* Lemmatization improves text understanding: By reducing words to their base form, Lemmatization makes it easier to understand the context and meaning of a sentence.\n",
        "\n",
        "* Lemmatization supports multiple languages: While Stemming may only work well for English, Lemmatization is effective for many different languages, making it a more versatile text processing technique."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the required resource for POS tagging\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# POS Taging\n",
        "# tokenize the text into words before POS Taging\n",
        "dff_new['pos_tags'] = dff_new['content_detail'].apply(nltk.word_tokenize).apply(nltk.pos_tag)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "dff_new.head(5)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Creating instance\n",
        "tfidfv = TfidfVectorizer(max_features=20000)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assuming dff_new is your DataFrame\n",
        "\n",
        "# 1. Fill NaN values with empty string\n",
        "dff_new['content_detail'] = dff_new['content_detail'].fillna('')\n",
        "\n",
        "# 2. Ensure all values are strings\n",
        "dff_new['content_detail'] = dff_new['content_detail'].astype(str)\n",
        "\n",
        "# 3. Define a custom tokenizer or modify token_pattern\n",
        "#    to include single-character words if necessary\n",
        "#    Here, we are using a simple tokenizer that splits\n",
        "#    on spaces and punctuation, and includes single-character words.\n",
        "def custom_tokenizer(text):\n",
        "    import re\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower()) # Match one or more word characters\n",
        "\n",
        "# 4. Create instance of TfidfVectorizer with the custom tokenizer\n",
        "tfidfv = TfidfVectorizer(max_features=30000, tokenizer=custom_tokenizer)\n",
        "\n",
        "# 5. Fit and transform the data\n",
        "x = tfidfv.fit_transform(dff_new['content_detail'])\n",
        "\n",
        "# 6. Check the shape\n",
        "print(x.shape)\n"
      ],
      "metadata": {
        "id": "XMXMbSCwmUwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF vectorization has been adopted instead of the traditional Bag of Words approach due to its ability to capture the significance of individual words within a document. By considering both term frequency and inverse document frequency, TF-IDF assigns greater weights to rare words that are exclusive to a specific document, thereby emphasizing their importance in the overall representation."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In textual data processing, there are 20,000 atrributes are created in text vectorization and this huge amount of columns cannot be dealed with our local machines. So, we will using the Principal Component Analysis(PCA) techniques to reduce the dimensions of this huge sparse matrix.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6TAvn99foFPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction\n",
        "# Importing PCA from sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "pca=PCA()\n",
        "pca.fit(x.toarray())\n",
        "\n",
        "variance= pca.explained_variance_ratio_\n",
        "print(f\"Explained variance: {variance}\")"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploting the percent of variance captured versus the number of components in order to determine the reduced dimensions\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(1, len(variance)+1), np.cumsum(pca.explained_variance_ratio_))\n",
        "ax.set_xlabel('Number of Components')\n",
        "ax.set_ylabel('Percent of Variance Captured')\n",
        "ax.set_title('PCA Analysis')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xe0xFAh9oY1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot demonstrates that 7770 principal components can capture 100% of the variance. However, for our case, we will focus on selecting the number of principal components that can capture 95% of the variance, ensuring a more efficient and practical representation of the data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SnKcWKUAoaMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we are passing the argument so that we can capture 95% of variance.\n",
        "# Defining instance\n",
        "pca_tuned = PCA(n_components=0.95)\n",
        "\n",
        "# Fitting and transforming the model\n",
        "pca_tuned.fit(x.toarray())\n",
        "x_transformed = pca_tuned.transform(x.toarray())\n",
        "\n",
        "# Checking the shape of transformed matrix\n",
        "x_transformed.shape"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)\n"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The utilization of Principal Component Analysis (PCA) as a technique for dimensionality reduction has proven highly valuable. By identifying the principal components that encapsulate the maximum variation within the data, PCA effectively condenses high-dimensional datasets while preserving the essential information. This popular method is favoured for its simplicity, computational efficiency, and wide accessibility in data analysis software. Moreover, PCA's extensive study and robust theoretical foundation make it a reliable and well-established approach."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "model = KMeans(random_state=0)\n",
        "visualizer = KElbowVisualizer(model,k=(1,15),locate_elbow=False)\n",
        "\n",
        "visualizer.fit(x_transformed)\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it seems that the elbow is forming at the 2 clusters but before blindly believing it let's plot one more chart that iterates over the same number of clusters and determines the Silhouette Score at every point.\n",
        "\n",
        "The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters. It is used to evaluate the quality of clustering, where a higher score indicates that objects are more similar to their own cluster and dissimilar to other clusters.\n",
        "\n",
        "The silhouette score ranges from -1 to 1, where a score of 1 indicates that the object is well-matched to its own cluster, and poorly-matched to neighboring clusters. Conversely, a score of -1 indicates that the object is poorly-matched to its own cluster, and well-matched to neighboring clusters.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OItwos6GqMQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Instantiate the clustering  visualizer\n",
        "visualizer = KElbowVisualizer(model,k=(2,10),metric='silhouette',timings=True,locate_elbow=False)\n",
        "\n",
        "visualizer.fit(x_transformed)\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "UkK0QQH8qoxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing Silhouette score for each k\n",
        "from sklearn.metrics import silhouette_score\n",
        "k_range = range(2,8)\n",
        "for k in k_range:\n",
        "  Kmodel = KMeans(n_clusters=k)\n",
        "  labels = Kmodel.fit_predict(x_transformed)\n",
        "  score = silhouette_score(x,labels)\n",
        "  print(f'k={k}, Silhouette score={score:.6f}')\n"
      ],
      "metadata": {
        "id": "JVfkuq9aq9P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plots (Elbow plot and Sillhouette plot) it is very clear that the Silhoutte score is comparatively good for 4 number of clusters, so we will consider 4 cluster in kmeans analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "3dgItijSrT9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the K-means model on a dataset\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++', random_state= 0)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=100)\n",
        "label = kmeans.fit_predict(x_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x_transformed[label == i , 0] , x_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WpJeSkoCrLae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the above figure in 3D using mplot3d library and see if we are getting the separated clusters.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fuz9pRgjrjru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library to visualize clusters in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot the clusters in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r', 'g', 'b', 'y']\n",
        "for i in range(len(colors)):\n",
        "    ax.scatter(x_transformed[kmeans.labels_ == i, 2], x_transformed[kmeans.labels_ == i, 0], x_transformed[kmeans.labels_ == i, 1], c=colors[i])\n",
        "\n",
        "# Rotate the plot 30 degrees around the X axis and 45 degrees around the Z axis\n",
        "ax.view_init(elev=20, azim=-120)\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0OTZNefBrmtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dateframe.\n",
        "dff_new['kmeans_cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "W4cSDRHVr-1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance"
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "def kmeans_wordcloud(cluster_number, column_name):\n",
        "    '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "    #Importing libraries\n",
        "    from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "    # Filter the data by the specified cluster number and column name\n",
        "    dff_wordcloud = dff_new[['kmeans_cluster', column_name]].dropna()\n",
        "    dff_wordcloud = dff_wordcloud[dff_wordcloud['kmeans_cluster'] == cluster_number]\n",
        "    dff_wordcloud = dff_wordcloud[dff_wordcloud[column_name].str.len() > 0]\n",
        "\n",
        "    # Combine all text documents into a single string\n",
        "    text = \" \".join(word for word in dff_wordcloud[column_name])\n",
        "\n",
        "    # Create the word cloud\n",
        "    wordcloud = WordCloud(stopwords=set(STOPWORDS)).generate(text)\n",
        "\n",
        "    # Convert the wordcloud to a numpy array\n",
        "    image_array = wordcloud.to_array()\n",
        "\n",
        "    # Return the numpy array\n",
        "    return image_array"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(20, 15))\n",
        "for i in range(4):\n",
        "    for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(kmeans_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nAinXrawswR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# HIERARCHICAL CLUSTERING\n",
        "distances_linkage = linkage(x_transformed, method = 'ward', metric = 'euclidean')\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('All films/TV shows')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "\n",
        "dendrogram(distances_linkage, no_labels = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A dendrogram is a tree-like diagram that records the sequences of merges or splits.More the distance of the vertical lines in the dendrogram, more the distance between those clusters.\n",
        "* From the above Dendogram we can say that optimal value of clusters is 2. But before assigning the values to respective clusters, let's check the silhouette scores using Agglomerative clustering and follow the bottom up approach to aggregate the datapoints.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZyFRGOzyt3qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Range selected from dendrogram above\n",
        "k_range = range(2,6)\n",
        "for k in k_range:\n",
        "    model = AgglomerativeClustering(n_clusters=k)\n",
        "    labels = model.fit_predict(x_transformed)\n",
        "    score = silhouette_score(x, labels)\n",
        "    print(\"k=%d, Silhouette score=%f\" % (k, score))"
      ],
      "metadata": {
        "id": "gmg668bguZxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above silhouette scores it is clear that the 2 clusters are optimal value (maximum Silhouette score), which is also clear from the above Dendogram that for 2 cluters the euclidean distances are maximum.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "svzafT39umcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the K-means model on a dataset\n",
        "Agmodel = AgglomerativeClustering(n_clusters = 2, linkage = 'ward') # Remove affinity parameter\n",
        "\n",
        "#predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = Agmodel.fit_predict(x_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x_transformed[label == i , 0] , x_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y6gTz1pruv3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "plotting the 3 Dimensional plot to see the clusters clearly."
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library to visualize clusters in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot the clusters in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r', 'g', 'b', 'y']\n",
        "for i in range(len(colors)):\n",
        "    ax.scatter(x_transformed[Agmodel.labels_ == i, 0], x_transformed[Agmodel.labels_ == i, 1], x_transformed[Agmodel.labels_ == i, 2],c=colors[i])\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dateframe.\n",
        "dff_new['agglomerative_cluster'] = Agmodel.labels_"
      ],
      "metadata": {
        "id": "2EZf-O0cfEf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "def agglomerative_wordcloud(cluster_number, column_name):\n",
        "  '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "  #Importing libraries\n",
        "  from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "  # Filter the data by the specified cluster number and column name\n",
        "  dff_wordcloud = dff_new[['agglomerative_cluster', column_name]].dropna()\n",
        "  dff_wordcloud = dff_wordcloud[dff_wordcloud['agglomerative_cluster'] == cluster_number]\n",
        "\n",
        "  # Combine all text documents into a single string\n",
        "  text = \" \".join(word for word in dff_wordcloud[column_name])\n",
        "\n",
        "  # Create the word cloud\n",
        "  wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "  # Return the word cloud object\n",
        "  return wordcloud"
      ],
      "metadata": {
        "id": "ZgUgo8pvfYn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(20, 15))\n",
        "for i in range(2):\n",
        "    for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(agglomerative_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w2nmW8gKfpts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using Cosine similarity as it is a measure of similarity between two non-zero vectors in a multidimensional space. It measures the cosine of the angle between the two vectors, which ranges from -1 (opposite direction) to 1 (same direction), with 0 indicating orthogonality (the vectors are perpendicular to each other).\n",
        "\n",
        "In this project we have used cosine similarity which is used to determine how similar two documents or pieces of text are. We represent the documents as vectors in a high-dimensional space, where each dimension represents a word or term in the corpus. We can then calculate the cosine similarity between the vectors to determine how similar the documents are based on their word usage.\n",
        "\n",
        "We'll are using cosine similarity over tf-idf because:\n",
        "\n",
        "Cosine similarity handles high dimensional sparse data better.\n",
        "\n",
        "Cosine similarity captures the meaning of the text better than tf-idf. For example, if two items contain similar words but in different orders, cosine similarity would still consider them similar, while tf-idf may not. This is because tf-idf only considers the frequency of words in a document and not their order or meaning.\n",
        "\n"
      ],
      "metadata": {
        "id": "-Onb1xykyhsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming that you want to apply it on the columns: 'description', 'listed_in', 'country', 'title'\n",
        "# Create a new column 'content_detail' by combining these 4 columns\n",
        "dff_new['content_detail'] = dff_new['description'] + ' ' + dff_new['listed_in'] + ' ' + dff_new['country'] + ' ' + dff_new['title']\n",
        "\n",
        "# Create a TF-IDF vectorizer object with adjusted parameters to handle potential issues:\n",
        "tfidf = TfidfVectorizer(stop_words='english', # Remove common English stop words\n",
        "                        min_df=2,  # Ignore terms appearing in fewer than 2 documents\n",
        "                        max_df=0.95, # Ignore terms appearing in more than 95% of documents\n",
        "                       )\n",
        "tfidf_matrix = tfidf.fit_transform(dff_new['content_detail'])\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "def recommend_content(title, cosine_sim=cosine_sim, data=dff_new):\n",
        "    # Get the index of the input title in the programme_list\n",
        "    programme_list = data['title'].to_list()\n",
        "    index = programme_list.index(title)\n",
        "\n",
        "    # Create a list of tuples containing the similarity score and index\n",
        "    # between the input title and all other programmes in the dataset\n",
        "    sim_scores = list(enumerate(cosine_sim[index]))\n",
        "\n",
        "    # Sort the list of tuples by similarity score in descending order\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]\n",
        "\n",
        "    # Get the recommended movie titles and their similarity scores\n",
        "    recommend_index = [i[0] for i in sim_scores]\n",
        "    rec_movie = data['title'].iloc[recommend_index]\n",
        "    rec_score = [round(i[1], 4) for i in sim_scores]\n",
        "\n",
        "    # Create a pandas DataFrame to display the recommendations\n",
        "    rec_table = pd.DataFrame(list(zip(rec_movie, rec_score)), columns=['Recommendation', 'Similarity_score(0-1)'])\n",
        "\n",
        "    return rec_table"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing indian movie\n",
        "recommend_content('Kuch Kuch Hota Hai')"
      ],
      "metadata": {
        "id": "D08ujXhfy4kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing non indian movie\n",
        "recommend_content('Hush')"
      ],
      "metadata": {
        "id": "ktSZvRW_y5or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing indian tv show\n",
        "recommend_content('Khaani')"
      ],
      "metadata": {
        "id": "XXdaEMjpy9VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing non indian tv show\n",
        "recommend_content('Balto')"
      ],
      "metadata": {
        "id": "faooLeaSzA_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have chosen Silhoutte Score over Distortion Score (also known as inertia or sum of squared distances) as evaluation metrics as it measures how well each data point in a cluster is separated from other clusters. It ranges from -1 to 1, with higher values indicating better cluster separation. A silhouette score close to 1 indicates that the data point is well-matched to its own cluster and poorly matched to neighboring clusters. A score close to 0 indicates that the data point is on or very close to the boundary between two clusters. A score close to -1 indicates that the data point is probably assigned to the wrong cluster.\n",
        "\n",
        "The advantages of using silhouette score over distortion score are:\n",
        "\n",
        "* Silhouette score takes into account both the cohesion (how well data points within a cluster are similar) and separation (how well data points in different clusters are dissimilar) of the clusters, whereas distortion score only considers the compactness of each cluster.\n",
        "* Silhouette score is less sensitive to the shape of the clusters, while distortion score tends to favor spherical clusters, and in our case the clusters are not completely spherical.\n",
        "* Silhouette score provides more intuitive and interpretable results, as it assigns a score to each data point rather than just a single value for the entire clustering solution."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have considered K-means as our final model, as we are getting the comparatevely high Silhoutte Score in K-means clustering and the resulted clusters are very well seperated from each others as have saw in the 3 dimensions."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on the exploratory data analysis (EDA), we can derive the following conclusions:**\n",
        "\n",
        "* Movies make up the majority of Netflix's content, accounting for 72%, while TV shows represent 28%.\n",
        "* The most common categories in Netflix's content library are Adult and Teen.\n",
        "* TV shows provide a larger variety of family-friendly content compared to movies.\n",
        "* Raul Campos and Jan Suter are the most frequent directors of movies, while Alastair Fothergill and Ken Burns are prominent directors of TV shows.\n",
        "\n",
        "**From a machine learning perspective, the following insights were observed:**\n",
        "\n",
        "* K-Means Clustering identifies 4 as the optimal number of clusters, while Agglomerative Hierarchical Clustering suggests 2 clusters.\n",
        "* The Silhouette Score was used to evaluate the clustering results, providing a more intuitive and clear understanding of the model’s performance.\n",
        "* A recommendation system was developed using similarity scores to offer personalized suggestions, enhancing user experience and reducing the risk of subscriber churn.\n",
        "\n",
        "**Additional insights from the analysis include:**\n",
        "\n",
        "* Most of the movie actors on Netflix are from India, but popular Indian actors are rarely featured in TV shows.\n",
        "* International content is the most popular genre on Netflix, followed by Drama and Comedy.\n",
        "* The United States and India are the leading contributors to Netflix's content library.\n",
        "* 2019 saw the highest number of movie additions, while 2020 had the highest number of TV show additions, highlighting a period of rapid content expansion and diversity.\n",
        "* The majority of movies on Netflix have a runtime of 90 to 120 minutes.\n",
        "* Adult content is predominantly produced by all countries except India, which primarily produces Teen content, while Canada excels in producing family-friendly content. Spain leads with the highest percentage (85%) of adult content.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}